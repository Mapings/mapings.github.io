## Reference

[Enhanced LSTM for Natural Language Inference](https://arxiv.org/abs/1609.06038)

Chen, Qian, et al. "Enhanced lstm for natural language inference." arXiv preprint arXiv:1609.06038 (2016).

## Introduction

The Enhanced Sequential Inference Model has 3 compnents:

- Input Encoding: use bi-LSIM to encode the input text

- Calculate attention weights: use dot product to calculate the attention weights btw two words，

<p align="center">
<img src="http://chart.googleapis.com/chart?cht=tx&chl=a^T*b"/>
</p>

- apply attention with weights

- Enhancement of local inference information

- The composition layer： keep using BiLSTM to compose local inference information sequentially

- pooling

- MLP

## Implementation on fluid

### Plan A

- Input: seq1, seq2 is lod tensor, label is int64 id

- bi-LSTM: use fluid.dynamic_lstm layer

- lod_tensor to tensor: use seqence_pad

- calculate attention weight: use matmul layer

- apply attention weights: use softmax layer

- tensor to lod_tensor: Waiting for Yibing

- bi-LSTM: use fluid.dynamic_lstm layer

- fluid.seqence_pool

- fluid.fc

### Plan B

- Input: seq1, seq2 is lod tensor, label is int64 id

- bi-LSTM: use fluid.dynamic_lstm layer

- Apply soft-aligned Attention on each seqence

- bi-LSTM: use fluid.dynamic_lstm layer

- fluid.seqence_pool

- fluid.fc
